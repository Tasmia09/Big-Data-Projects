{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this all about?\n",
    "\n",
    "In order to run Spark applications on your local machine, you must have **Java 8**, **Spark**, and the **PySpark** package installed.  Additionally, for the Jupyter Notebook kernel to use your locally installed **Spark**, you may need to use the **findspark** module.   If you are unsure whether you have any or all of these requirements, we recommend you follow the instructions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Java Installation\n",
    "\n",
    "Executing the following cell will install **Java 8** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/java/`).  If you would rather install Java yourself, do not execute the following cell and download Java and follow the instructions for your operating system found on the [Java website](https://www.java.com/en/download/).\n",
    "\n",
    "*Note: If you have Java already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/java/` and add the installed Java directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit)*\n",
    "\n",
    "*Mac OSX: The code to install Java may or may not work on your machine.  If you experience an error, please download Java and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform as arch\n",
    "from sys import platform\n",
    "   \n",
    "print('Beginning Java 8 installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if platform.startswith('linux'):\n",
    "    print('Now installing Java on Linux...')\n",
    "    if arch.architecture()[0] == '64bit':\n",
    "        !curl -o ~/java.tar.gz -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234464_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    elif arch.architecture()[0] == '32bit':\n",
    "        !curl -o ~/java.tar.gz -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234462_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    !rm -rf ~/java && mkdir ~/java && tar -xzf ~/java.tar.gz -C ~/java --strip-components=1 && rm ~/java.tar.gz\n",
    "\n",
    "    # Define JAVA_HOME and add to PATH\n",
    "    !echo 'export JAVA_HOME=$HOME/java' >> ~/.bashrc\n",
    "    !echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Java 8 complete!')\n",
    "\n",
    "elif platform == 'darwin':\n",
    "    print('Now installing Java on Mac...')\n",
    "    !curl -o ~/java.dmg -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234465_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    !hdiutil attach ~/java.dmg\n",
    "    !sudo installer -pkg /Volumes/Java\\ 8\\ Update\\ 181/Java\\ 8\\ Update\\ 181.app/Contents/Resources/JavaAppletPlugin.pkg -target /\n",
    "    !diskutil umount /Volumes/Java\\ 8\\ Update\\ 181 \n",
    "    print('Installation of Java 8 complete (maybe)... If there was an error, please mount java.dmg located in your home directory and follow the instructions to install')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Java from here: https://java.com/en/download/manual.jsp and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Spark Installation\n",
    "\n",
    "Executing the following cell will install the latest **Apache Spark** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/spark/`).\n",
    "\n",
    "If you would rather install Spark yourself, do not execute the following cell and download the pre-built version of Spark found on the [Spark website](https://spark.apache.org/downloads.html).  You will need to extract the contents of the tarball and add Spark to your `PATH` variable.\n",
    "\n",
    "*Note: If you have Spark already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/spark/` and add the installed Spark directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit).*\n",
    "\n",
    "*Mac OSX: The code has been tested and should work for modern Macs.*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Spark website](https://spark.apache.org/downloads.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "\n",
    "print('Beginning Apache Spark installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if (platform.startswith('linux')) or (platform == 'darwin'):\n",
    "    # Download Spark and extract into $HOME/spark/\n",
    "    !curl -o ~/spark.tar.gz -L http://apache.cs.utah.edu/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n",
    "    !rm -rf ~/spark && mkdir ~/spark && tar -xzf ~/spark.tar.gz -C ~/spark --strip-components=1 && rm ~/spark.tar.gz\n",
    "    \n",
    "    # Define SPARK_HOME and add to PATH\n",
    "    !echo 'export SPARK_HOME=$HOME/spark' >> ~/.bashrc\n",
    "    !echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    \n",
    "    # Set spark master to localhost (may not be necessary)\n",
    "    !echo 'export SPARK_LOCAL_IP=\"127.0.0.1\"' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Apache Spark complete!')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Spark from here: https://spark.apache.org/downloads.html and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Pyspark Installation\n",
    "\n",
    "Executing the following cell will install **pyspark** and **findspark**.  We will use either `conda` (if you have anaconda3 or miniconda installed) or `pip`.  At the minimum, you must have `pip` installed.  You likely have one of these installed already!  If you do not, you can download and install [Anaconda3](https://www.anaconda.com/download/) or [pip](https://pip.pypa.io/en/stable/installing/).  We will also install **matplotlib** so you can plot results from your assignments and pre-build the font cache to save time in the future!\n",
    "\n",
    "*Note: These instructions should work regardless of the OS you are running!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Method to build Matplotlib font cache\n",
    "def buildFontCache():\n",
    "    import matplotlib\n",
    "    matplotlib.use('AGG')\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.plot([0],[0])\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# Check for conda\n",
    "if shutil.which('conda'):\n",
    "    # Update conda\n",
    "    !conda update -n base conda --yes\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !conda install pyspark --yes\n",
    "    !conda install -c conda-forge findspark --yes\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !conda install matplotlib --yes\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python package installation complete!')\n",
    "\n",
    "    \n",
    "# Check for pip if conda is not found\n",
    "elif shutil.which('pip'):\n",
    "    # Update pip\n",
    "    !pip install --upgrade pip\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !pip install pyspark\n",
    "    !pip install findspark\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !pip install matplotlib\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python packages installation complete!')\n",
    "    \n",
    "else:\n",
    "    print('Could not find conda or pip, please follow the instructions above to install either Anaconda3 or pip.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it Work?\n",
    "\n",
    "Let's test that **Java**, **Spark**, **pyspark**, and **findspark** were all installed correctly.  The following should create a `SparkContext`, create an `RDD` from a python list, and print the values in the `RDD`.\n",
    "\n",
    "**Expected output:**\n",
    "`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = sc.parallelize(range(10))\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the output of the previous cell did not match the expected output or you received an error message, please ask for assistance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark Code\n",
    "\n",
    "Now that you have all the software installed to run **Spark** code in a Jupyter Notebook, keep in mind that you will need to use the following code at the beginning of each notebook where you wish to use **Spark**.  This initialization code will create a `SparkContext` which you can access via `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
