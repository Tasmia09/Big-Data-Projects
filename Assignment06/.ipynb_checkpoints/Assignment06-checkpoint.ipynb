{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 594 / CS 690 - Assignment 06\n",
    "### October 8, 2018\n",
    "---\n",
    "\n",
    "For this assignment, you must work in groups of one or two students. Each person is responsible to write their own code, but the group will (together) discuss their solution.  In this notebook, we provide you with basic functions for completing the assignment.  *Complete the assignment in this notebook.  You will need to modify existing code and write new code to find a solution*.  Each member of the group must upload their own work (i.e., a notebook file) to GitHub.\n",
    "\n",
    "*Note: Running a cell will not rerun previous cells.  If you edit code in previous cells, you must rerun those cells.  We recommend using* `Run All` *to avoid any errors results from not rerunning previous cells.  You can find this in the menu above:* `Cell -> Run All`\n",
    "\n",
    "During today's lecture, we learned about $k$-means clusterings. In the previous assignment, we learned to use PySpark's parallel versions of the `map` and `reduce` functions. In this assignment, we will be implementing the $k$-means algorithm in parallel with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "Run the cell below to verify that Java, Spark, and PySpark are successfully installed. The cell generates a dataset of numbers (i.e., 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10) and computes their sum. The expected output is 45. If you run into an error, return to the Spark-Install scripts from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(range(1,10))\n",
    "print(data.reduce(lambda x,y: x+y))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "**Implement $k$-means in spark.**\n",
    "\n",
    "Below we have provided you with several helper functions that can be used to implement $k$-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add value1 and value 2\n",
    "# Useful as a reduce function\n",
    "def addValues(val1, val2):\n",
    "    return val1 + val2\n",
    "\n",
    "# Calculate the euclidian distance between two 2D points\n",
    "# HINT: ref 1\n",
    "# Input: point_a: np.array(x,y)\n",
    "#        point_b: np.array(x,y)\n",
    "# Return: distance\n",
    "def dist(point_a, point_b):\n",
    "    return sum (np.sqrt((point_a - point_b)**2))\n",
    "\n",
    "# Find the centroid that the `point` is closest to and return the centroid's ID\n",
    "# The centroid ID in this case is simply its index in the `centroids` list\n",
    "# Input: point: np.array(x,y)\n",
    "#        centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)], \n",
    "#                   where K is the number of clusters\n",
    "# Return: clusterID\n",
    "def getClosestCentroidID(point, centroids):\n",
    "    distances = [dist(point, centroid) for centroid in centroids]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# Convert the given `line` to a point\n",
    "# As in assignment 4, we recommend using numpy arrays to store the (x,y)-coordinates of the points\n",
    "# Input: line: \"float float\"\n",
    "# Return: point: np.array(x,y)\n",
    "def lineToPoint(line):\n",
    "    return np.array([float(x) for x in line.split()])\n",
    "\n",
    "# Given a point (i.e., (x,y) and a list of centroids (i.e., list of points),\n",
    "# find the closest centroid and assign that cluster to the point\n",
    "# Input: points_rdd: <<np.array(x1,y1), np.array(x2,y2), ... np.array(xN,yN)>>,\n",
    "#                    where N is the number of lines in the file\n",
    "#        centroids:  [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                    where K is the number of clusters\n",
    "# Return: RDD of clustered points: <<(clusterID, np.array(x1, y1)), (clusterID, np.array(x2, y2)), ...>>\n",
    "def assignPointsToClosestCluster(points_rdd, centroids):\n",
    "    return points_rdd.map(lambda x: (getClosestCentroidID(x, centroids), x))\n",
    "    \n",
    "# Read in the file and convert each line into a point (using `lineToPoint`) with Spark\n",
    "# Return: RDD of points: <<np.array(x1,y1), np.array(x2,y2), ... np.array(xN,yN)>>,\n",
    "#                        where N is the number of lines in the file\n",
    "def readPointsFromFile(filename):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    lines = sc.textFile(filename)\n",
    "    points = lines.map(lineToPoint)\n",
    "    return points\n",
    "\n",
    "# Sum the distance that each centroid moved by\n",
    "# Input: old_centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                       where K is the number of clusters\n",
    "#        new_centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                       where K is the number of clusters\n",
    "# Return: sum of distances\n",
    "def calculateChangeInCentroids(old_centroids, new_centroids):\n",
    "    return sum([dist(old, new) for old, new in zip(old_centroids, new_centroids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [1: euclidian distance for two dimensions](https://en.wikipedia.org/wiki/Euclidean_distance#Two_dimensions)\n",
    "\n",
    "----\n",
    "\n",
    "In the next cell, you will implement the key components of the $k$-means clustering algorithm.  The first piece of the algorithm is calculating the mean coordinates of each cluster. In plain terms, to calculate the mean (i.e., average) $x$-coordinate of Cluster $j$, you must first find all of the points in the cluster. Then you must sum their $x$-coordinate values.  Finally, you divide that sum by the number of points in the cluster.  This produces the mean $x$-coordinate.  You must repeat this process for the $y$-coordinate (or for greater efficiency, you can perform it in parallel with the $x$-coordinate calculation). The equation below demonstrates how to calculate the mean coordinates of a given cluster:\n",
    "\n",
    "$$\\overline{Cluster_j} = \\left\\langle \\frac{x_1 + x_2 + \\ldots + x_n}{n} , \\frac{y_1 + y_2 + \\ldots + y_n}{n} \\right\\rangle$$ where $$n = \\left\\vert Cluster_j \\right\\vert$$ and $$\\langle x_1, y_1\\rangle, \\langle x_2, y_2\\rangle, \\ldots, \\langle x_n, y_n\\rangle \\in Cluster_j$$\n",
    "\n",
    "Your task is to **implement the `calculateClusterMeans` function and then test it against our provided test case**.  You will need to calculate the expected output by hand to ensure that your function produces the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.75, 1.5 ]), array([2. , 3.5])]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean coordinates of each cluster.\n",
    "# Input: clustered_points_rdd: <<clustered_point1, clustered_point2, ..., clustered_pointN>>,\n",
    "#                              where N is the number of clustered_points, and\n",
    "#                              where each clustered_point looks like (clusterID, np.array(x,y))\n",
    "# Return: [centroid1, centroid2, ..., centroidK],\n",
    "#         where K is the number of clusters, and\n",
    "#         where each centroid is np.array(x,y)\n",
    "def calculateClusterMeans(clustered_points_rdd):\n",
    "    # Sum the xs and ys of all points in each cluster\n",
    "    # HINT: ref 1\n",
    "    sum_x_y = clustered_points_rdd.reduceByKey(addValues)\n",
    "    count_points = clustered_points_rdd.countByKey().items()\n",
    "    sum_x_y_list = sum_x_y.collect()\n",
    "    # Count the number of points in each cluster\n",
    "    # HINT: ref 2\n",
    "    #print(count_points)\n",
    "    #print(sum_x_y_list)\n",
    "    x = 0\n",
    "    \n",
    "    newList = []\n",
    "    \n",
    "    for key, value in count_points:\n",
    "        newList.append(np.array([x / value for x in sum_x_y_list[x][1]]))\n",
    "        x+=1\n",
    "    \n",
    "    # Divide the x,y sums for each cluster by the number of points in each cluster\n",
    "    \n",
    "   # Divide the x,y sums for each cluster by the number of points in each cluster\n",
    "    cluster_means = newList\n",
    "   \n",
    "    return cluster_means\n",
    "    \n",
    "    \n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "test_clustered_points = sc.parallelize([(0, np.array([1.0, 2.0])),\n",
    "                                        (0, np.array([0.5, 1.0])),\n",
    "                                        (1, np.array([4.0, 8.0])),\n",
    "                                        (1, np.array([0.0, -1.0]))\n",
    "                                       ])\n",
    "print(calculateClusterMeans(test_clustered_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [1: reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)\n",
    "- [2: countByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByKey)\n",
    "\n",
    "----\n",
    "\n",
    "You now have all the tools you need to write the main $k$-means function.  In this function, **your task is to read in the points from the provided file, cluster the points into $K$ clusters, and then return the clustering results (cluster centroids and clustered points)**.   Your code should accept two arguments: 1) the filename of the file containing the data that should be clustered and 2) the number of clusters to create. The input files we provide (`contig.txt` and `mickey.txt`) have one data point per line; the coordinates of the data points are two space delimited floating point numbers (i.e., $x$ and $y$). Your code should output the final centroids of each cluster as well as each point and its assigned cluster. Remember to use the functions that we have given you and that you have implemented in the previous cells (i.e., `readPointsFromFile`, `assignPointsToClosestCluster`, `calculateClusterMeans`, `assignPointsToClosestCluster`, and `calculateChangeInCentroids`.\n",
    "\n",
    "We have provided you with a very small, simple dataset to test against (i.e., `simple.txt`). This dataset is small enough that it can be printed out in its entirety.  It contains two very distinct clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered Data\n",
      "\tCluster 0:\n",
      "\t\tCoordinates: [-4.44375343  0.        ]\n",
      "\t\tCoordinates: [-4.42332439  0.        ]\n",
      "\t\tCoordinates: [-4.21117849  0.        ]\n",
      "\t\tCoordinates: [-4.19834549  0.        ]\n",
      "\t\tCoordinates: [-4.13252583  0.        ]\n",
      "\tCluster 1:\n",
      "\t\tCoordinates: [3.38701979 0.        ]\n",
      "\t\tCoordinates: [3.56450093 0.        ]\n",
      "\t\tCoordinates: [3.67432789 0.        ]\n",
      "\t\tCoordinates: [3.76562207 0.        ]\n",
      "\t\tCoordinates: [3.88865504 0.        ]\n",
      "\t\tCoordinates: [4.20330179 0.        ]\n",
      "Cluster Centers\n",
      "\tCluster 0: [-4.28182553  0.        ]\n",
      "\tCluster 1: [3.74723792 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Cluster the points in the file into K clusters using k-means clustering\n",
    "# Input: filename: file that contains the points (one per line)\n",
    "#        K: number of clusters\n",
    "# Return: [centroid1, centroid2, ..., centroidK] and [clustered_point1, clustered_point2, ..., clustered_pointN]\n",
    "#         where K is the number of clusters,\n",
    "#         where N is the number of points,\n",
    "#         where centroid is np.array(x,y), and\n",
    "#         where each clustered_point is (clusterID, np.array(x,y))\n",
    "def clusterFile(filename, K):\n",
    "    # Read in the file and convert to points\n",
    "    points = readPointsFromFile(filename)\n",
    "            \n",
    "    \n",
    "    # Randomly choose K points as the centroids\n",
    "    # HINT: ref 1\n",
    "    centroids = [] #TODO\n",
    "    centroids = points.takeSample(False, K)\n",
    "    \n",
    "    # Assign each point to the centroid closest to it\n",
    "    closest_points = assignPointsToClosestCluster(points, centroids)\n",
    "    \n",
    "    # Begin the iterative portion of k-means,\n",
    "    # continue until the changes in centroids are very small (e.g., < .0001)\n",
    "    change_in_centroids = math.inf\n",
    "    while change_in_centroids > 0.0001:\n",
    "        old_centroids = centroids\n",
    "        # Calculate the new centroids based on the means of the current clusters\n",
    "        centroids = calculateClusterMeans(closest_points)\n",
    "        # Assign the points to the new centroids\n",
    "        closest_points = assignPointsToClosestCluster(points, centroids)\n",
    "        # Calculate the change in the centroids since the last iteration\n",
    "        change_in_centroids = calculateChangeInCentroids(old_centroids, centroids) #TODO\n",
    "\n",
    "    return centroids, closest_points.collect() #TODO\n",
    "\n",
    "centroids, clustered_points = clusterFile('simple.txt', 2)\n",
    "\n",
    "print(\"Clustered Data\")\n",
    "prev_cluster_id = -1\n",
    "for clustered_point in sorted(clustered_points, key=lambda x: x[0]):\n",
    "    cluster_id, point_coords = clustered_point\n",
    "    if cluster_id > prev_cluster_id:\n",
    "        print(\"\\tCluster {}:\".format(cluster_id))\n",
    "    prev_cluster_id = cluster_id\n",
    "    print(\"\\t\\tCoordinates: {}\".format(point_coords))\n",
    "\n",
    "print(\"Cluster Centers\")\n",
    "for idx, centroid in enumerate(centroids):\n",
    "    print(\"\\tCluster {}: {}\".format(idx, centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [1: takeSample](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeSample)\n",
    "\n",
    "----\n",
    "\n",
    "In the cell below, **your task is to devise a way to visualize the clusters you have created** (to verify that the algorithm works as expected on larger datasets).  We have provided you with code that plots each point (using `matplotlib`), but you must extend the code to plot the cluster centers as well as color each point based on the cluster it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the clustered points along with the cluster centers.\n",
    "# The points should be colored based on the cluster they are in.\n",
    "# Input: centroids: [(x1,y1), (x2,y2), ..., (xK,yK)], \n",
    "#                   where K is the number of clusters\n",
    "#        clusteredPoints: [(clusterID, (x1,y1)), (clusterID, (x2,y2)), ... (clusterID, (xN,yN))], \n",
    "#                         where N is the number of points\n",
    "def plotClusters(centroids, clustered_points, title=None):\n",
    "    if len(clustered_points) <= 0:\n",
    "        return\n",
    "    \n",
    "    # Extract the cluster ids and points from clusteredPoints\n",
    "    # See ref 1\n",
    "        \n",
    "    cluster_ids, points = zip(*clustered_points)\n",
    "    \n",
    "    # Extract x and y values from the points and centroids\n",
    "    point_xs, point_ys = zip(*points)\n",
    "    print(len(point_xs))\n",
    "    centroid_xs, centroid_ys = zip(*centroids)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.scatter(centroid_xs, centroid_ys, marker='*', s=200, color='black')\n",
    "    # Plot each cluster center as a black star (that is slightly larger\n",
    "    #      than the data points plotted below)\n",
    "    # HINT: ref 4  (specifically look at the 'marker', 's', and 'c' keyword args)\n",
    "    \n",
    "    # Pick a color to use for each cluster (e.g., cluster1 is \"blue\", cluster2 is \"red\", ...)\n",
    "    # HINT: ref 2, ref 3\n",
    "    cluster_points_map_x = dict()\n",
    "    cluster_points_map_y = dict()\n",
    "    key = cluster_ids[0]\n",
    "    for x in range(len(cluster_ids)):\n",
    "        key = cluster_ids[x]\n",
    "        cluster_points_map_x.setdefault(key, [])\n",
    "        cluster_points_map_y.setdefault(key, [])\n",
    "        cluster_points_map_x[key].append(point_xs[x])        \n",
    "        cluster_points_map_y[key].append(point_ys[x])\n",
    "    \n",
    "    # Plot each point in the figure using a scatter plot\n",
    "    # Each point should be colored based on the cluster that it is in\n",
    "    # HINT: ref 4 (specifically look at the 'marker', 'c', 'cmap', and 'norm' keyword args)\n",
    "    # The 'zorder' keyword argument is used here to make sure the points are drawn behind\n",
    "    #      the cluster center stars\n",
    "    color = ['blue','red', 'green', 'yellow', 'orange']\n",
    "    for x in range(len(cluster_points_map_x)):\n",
    "        plt.scatter(cluster_points_map_x[x], cluster_points_map_y[x], marker='o', s=5, color=color[x])\n",
    "    #plt.scatter(point_xs, point_ys, marker='o', zorder=-1)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "centroids, clustered_points = clusterFile('mickey.txt', 3)\n",
    "plotClusters(centroids, clustered_points, title=\"Mickey\")\n",
    "#centroids, clustered_points = clusterFile('contig.txt', 5)\n",
    "#plotClusters(centroids, clustered_points, title=\"Contiguous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [1: zip(*)](https://stackoverflow.com/a/5917600)\n",
    "- [2: using normalization and colormaps in matplotlib](https://stackoverflow.com/questions/43132300/how-can-i-normalize-colormap-in-matplotlib-scatter-plot)\n",
    "- [3: colormap reference](https://matplotlib.org/examples/color/colormaps_reference.html)\n",
    "- [4: scatter](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html?highlight=matplotlib%20pyplot%20scatter#matplotlib.pyplot.scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider:\n",
    "* How do you choose $K$ for your dataset?\n",
    "  * A popular technique is the elbow method:\n",
    "    * https://pythonprogramminglanguage.com/kmeans-elbow-method/\n",
    "    * https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_elbow_method\n",
    "* Do you always get the same results from $k$-means?  Is it non-determinisitc?  Is this an error in your code or a feature of the algorithm?\n",
    "* How would you optimize the code to work for larger datasets (e.g., 100GBs of points)?\n",
    "* How would you generalize the code to work for larger-dimensionality datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Questions:\n",
    "**Answer the following questions, in a couple sentences each, in the cells provided below**\n",
    "* Do you work alone or in team? If in team, indicate who is your collaborator and what are the skills / expertise he/she is bringing to the project\n",
    "* What is the dataset you are considering? \n",
    "* What are the possible key question(s) you want to answer? Are the questions too general? Are the questions too narrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For the final project I am teaming up with Maria Mahbub and we are going to work on the dataset \"https://www.kaggle.com/new-york-city/nyc-parking-tickets\"\n",
    "The questions we will try to answer is \n",
    "1. What factors might have an impact on issuing ticket?\n",
    "2. Does it depend on the area?\n",
    "3. Any rapid occurence of features of cars?\n",
    "4. Can we build a prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Questions:\n",
    "**Answer the following questions, in a couple sentences each, in the cells provided below**\n",
    "* List the key tasks you accomplished during this assignment?\n",
    "* Describe the challenges you faced in addressing these tasks and how you overcame these challenges?\n",
    "* Did you work with other students on this assignment? If yes, how did you help them? How did they help you? Be as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I learnt to implement k-means clustering algorithm. The basics of centroid and clusters was a bit hard to grasp at\n",
    "first but as I followed the instructions, it became easy. I worked with other student as I was getting an error\n",
    "in clusterFile() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
